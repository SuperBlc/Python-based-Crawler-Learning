{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一章 爬虫基础\n",
    "\n",
    "\n",
    "## 什么是爬虫？\n",
    "\n",
    "爬虫就是请求网站并提取数据信息的自动化程序\n",
    "    \n",
    "## 爬虫基本流程\n",
    "\n",
    "1. 发起请求：通过HTTP库向目标站点发起请求，即发送一个request，请求可以包含额外的headers等信息，等待服务器响应；\n",
    "2. 获取响应内容：如果服务器能够正常响应，会得到一个Response。Response的内容便是所要获取的页面内容，类型可能有HTML，JSON字符串，二进制数据（如图片视频等）等类型；\n",
    "3. 解析内容：得到的内容可能是HTML，可以用正则表达式、网页解析库进行解析。可能是Json，可以直接转为Json对象解析，可能是二进制数据，可以保存或进一步处理；\n",
    "4. 保存数据：保存形式多样，可以是文本，也可以保存至数据库，或则保存成特定格式的文件。\n",
    "\n",
    "## url详解\n",
    "\n",
    "> URL(Uniform Resuorce Locator),统一资源定位符\n",
    "\n",
    "一个URL由以下几部分组成：\n",
    "\n",
    "```text\n",
    "scheme://host:port/path/?query-string=value#anchor\n",
    "```\n",
    "\n",
    "- scheme：代表的是访问协议，一般为http, https 以及 ftp 等\n",
    "- host：主机，域名\n",
    "- port：端口号\n",
    "- path：查找路径\n",
    "- query-string：查询字符串\n",
    "- anchor：锚点，前端用来做页面定位\n",
    "\n",
    "在浏览器中请求一个url，浏览器会对url进行编码，除英文字母，数字，部分符号外，其他的全部使用十六进制进行编码显示。\n",
    "\n",
    "## Request中的基本信息\n",
    "\n",
    "请求方式、请求的URL、请求头、请求体\n",
    "\n",
    "## Response中包含的信息\n",
    "\n",
    "状态码、响应头（如内容类型、内容长度、服务器信息、设置cookie等）、响应体（最主要的部分，包含了请求的资源内容，如网页HTML、图片二进制数据等）。\n",
    "\n",
    "## 抓取数据类型\n",
    "\n",
    "1. 网页文本，如HTML文档，Json格式的文本\n",
    "2. 图片，获取的是二进制文件，保存为相应的图片格式即可\n",
    "3. 视频，同为二进制文件，保存为相应的视频格式即可\n",
    "4. 其他，文档（.doc, .xls等）\n",
    "\n",
    "## 如何解决Javascript渲染的问题\n",
    "\n",
    "1. 分析Ajax请求\n",
    "2. 使用Selenium+WebDriver，模拟真人上网过程，浏览器将页面渲染完成后，再分析页面，提取数据。\n",
    "3. [splash](https://splash.readthedocs.io/en/stable/)，一个JavaScript渲染服务，功能类似于Selenium\n",
    "4. PyV8, Ghost.py\n",
    "\n",
    "## 保存数据的格式\n",
    "\n",
    "1. 文本：纯文本，Json，XML\n",
    "2. 关系型数据库：如MySQL，Oracle，SQL Server等具有结构化表结构形式的存储\n",
    "3. 非关系型数据库：如MongoDB，Redis等key-value形式存储\n",
    "4. 二进制文件：如图片、视频等。\n",
    "\n",
    "## 附录\n",
    "\n",
    "### 常见的请求方法\n",
    "\n",
    "在http协议中，定义了9种请求方法，分别为\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<img src=\".\\static\\image\\http请求方法.png\"/>\n",
    "    图片1 参考<a href=\"https://www.runoob.com/http/http-methods.html\">RUNOOB.COM-HTTP请求方法</a>\n",
    "</div>\n",
    "\n",
    "HTTP1.0定义了3种请求方法：GET, POST, HEAD方法。\n",
    "\n",
    "HTTP1.1新增了6种请求方法：OPTIONS, PUT, PATCH, DELETE, TRACE, CONNECT方法。\n",
    "\n",
    "常用的两种请求方法：GET和POST请求。\n",
    "\n",
    "1. GET请求：一般情况下，只从服务器获取数据，并不对服务器资源产生任何的影响。\n",
    "2. POST请求：向服务器发送数据，上传文件等，会对服务器资源产生影响。\n",
    "\n",
    "注意：要根据服务器的要求请求数据，否则因为网站的反爬虫机制，会被识别出是爬虫程序而被禁止访问资源。\n",
    "\n",
    "### 请求头常见参数\n",
    "\n",
    "在http协议中，向服务器发送一个请求，数据分为三部分：（1）将数据放在url中；（2）把数据放在body中（在post请求中）；（3）把数据放在head中。以下介绍一些常用的请求头参数：\n",
    "\n",
    "1. User-Agent：浏览器名称。这个在网络爬虫中会经常用到。请求一个网页的时候，服务器通过这个参数可以知道请求是由哪个浏览器发送的。如果我们通过爬虫发送请求，则我们的User-Agent就是Python，那些带有反爬虫机制的网站就会识别出来。因此我们常设置这个值（User-Agent）为浏览器的值，以此伪装爬虫。\n",
    "\n",
    "2. Refer：表明当前这个请求是从那个url发送过来的。这个一般也用来作为反爬虫机制。如果不是指定页面过来的，那么就不作相关响应。\n",
    "\n",
    "3. Cookie：HTTP协议是无状态的，也就是同一个人发送两次请求，服务器没有能力知道请求是否来自同一个人。因此这个时候就用cookie来做标识。一般如果想要登录后才能访问的网站，则需要发送cookie信息。\n",
    "\n",
    "### 常见的响应状态码\n",
    "\n",
    "1. 200：请求正常，服务器正常返回数据\n",
    "\n",
    "2. 301：永久重定向\n",
    "\n",
    "3. 302：临时重定向\n",
    "\n",
    "4. 400：请求的url服务器上找不到\n",
    "\n",
    "5. 403：服务器拒绝访问，权限不够\n",
    "\n",
    "6. 500：服务器内部错误\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
