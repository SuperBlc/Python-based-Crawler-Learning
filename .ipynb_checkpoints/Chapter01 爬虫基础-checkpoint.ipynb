{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一章 爬虫基础\n",
    "\n",
    "## url详解\n",
    "\n",
    "> URL(Uniform Resuorce Locator),统一资源定位符\n",
    "\n",
    "一个URL由以下几部分组成：\n",
    "\n",
    "```text\n",
    "scheme://host:port/path/?query-string=value#anchor\n",
    "```\n",
    "\n",
    "- scheme：代表的是访问协议，一般为http, https 以及 ftp 等\n",
    "- host：主机，域名\n",
    "- port：端口号\n",
    "- path：查找路径\n",
    "- query-string：查询字符串\n",
    "- anchor：锚点，前端用来做页面定位\n",
    "\n",
    "在浏览器中请求一个url，浏览器会对url进行编码，除英文字母，数字，部分符号外，其他的全部使用十六进制进行编码显示。\n",
    "\n",
    "## 常见的请求方法\n",
    "\n",
    "在http协议中，定义了9种请求方法，分别为\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<img src=\".\\static\\image\\http请求方法.png\"/>\n",
    "    图片1 参考<a href=\"https://www.runoob.com/http/http-methods.html\">RUNOOB.COM-HTTP请求方法</a>\n",
    "</div>\n",
    "\n",
    "HTTP1.0定义了3种请求方法：GET, POST, HEAD方法。\n",
    "\n",
    "HTTP1.1新增了6种请求方法：OPTIONS, PUT, PATCH, DELETE, TRACE, CONNECT方法。\n",
    "\n",
    "常用的两种请求方法：GET和POST请求。\n",
    "\n",
    "1. GET请求：一般情况下，只从服务器获取数据，并不对服务器资源产生任何的影响。\n",
    "2. POST请求：向服务器发送数据，上传文件等，会对服务器资源产生影响。\n",
    "\n",
    "注意：要根据服务器的要求请求数据，否则因为网站的反爬虫机制，会被识别出是爬虫程序而被禁止访问资源。\n",
    "\n",
    "## 请求头常见参数\n",
    "\n",
    "在http协议中，向服务器发送一个请求，数据分为三部分：（1）将数据放在url中；（2）把数据放在body中（在post请求中）；（3）把数据放在head中。以下介绍一些常用的请求头参数：\n",
    "\n",
    "1. User-Agent：浏览器名称。这个在网络爬虫中会经常用到。请求一个网页的时候，服务器通过这个参数可以知道请求是由哪个浏览器发送的。如果我们通过爬虫发送请求，则我们的User-Agent就是Python，那些带有反爬虫机制的网站就会识别出来。因此我们常设置这个值（User-Agent）为浏览器的值，以此伪装爬虫。\n",
    "\n",
    "2. Refer：表明当前这个请求是从那个url发送过来的。这个一般也用来作为反爬虫机制。如果不是指定页面过来的，那么就不作相关响应。\n",
    "\n",
    "3. Cookie：HTTP协议是无状态的，也就是同一个人发送两次请求，服务器没有能力知道请求是否来自同一个人。因此这个时候就用cookie来做标识。一般如果想要登录后才能访问的网站，则需要发送cookie信息。\n",
    "\n",
    "## 常见的响应状态码\n",
    "\n",
    "1. 200：请求正常，服务器正常返回数据\n",
    "\n",
    "2. 301：永久重定向\n",
    "\n",
    "3. 302：临时重定向\n",
    "\n",
    "4. 400：请求的url服务器上找不到\n",
    "\n",
    "5. 403：服务器拒绝访问，权限不够\n",
    "\n",
    "6. 500：服务器内部错误\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urllib库\n",
    "\n",
    "urllib是Python中最基本的网络请求库。可以模拟浏览器行为，向指定的服务器发送请求，并保存服务器返回的数据。\n",
    "\n",
    "## urlopen函数\n",
    "\n",
    "在Python3的urllib库中，所有和网站请求相关的方法，都集成到了```urllib.request```模块中，以下是基本实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\\n  \"args\": {}, \\n  \"headers\": {\\n    \"Accept-Encoding\": \"identity\", \\n    \"Host\": \"www.httpbin.org\", \\n    \"User-Agent\": \"Python-urllib/3.6\"\\n  }, \\n  \"origin\": \"117.173.217.126, 117.173.217.126\", \\n  \"url\": \"https://www.httpbin.org/get\"\\n}\\n'\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from urllib import request\n",
    "\n",
    "# urlopen的返回对象是一个http.client.HttpResponse对象，是一个类文件句柄\n",
    "req = request.urlopen('http://www.httpbin.org/get')\n",
    "print(req.read())\n",
    "print(req.getcode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
